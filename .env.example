# OpenAI API Configuration
# For OpenAI directly:
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4

# For OpenRouter (alternative):
# OPENAI_API_KEY=your_openrouter_api_key_here
# OPENAI_BASE_URL=https://openrouter.ai/api/v1
# OPENAI_MODEL=openai/gpt-4

# For LiteLLM proxy (alternative):
# OPENAI_API_KEY=your_api_key_here
# OPENAI_BASE_URL=http://localhost:4000
# OPENAI_MODEL=gpt-4

# Agent Configuration
AGENT_TIMEOUT=300
MAX_SHELL_OUTPUT_SIZE=10000
LOG_LEVEL=INFO

# Development/Testing Configuration
# Set to true to enable debug logging
DEBUG=false

# Working directory restriction for shell commands (optional)
# If not set, defaults to current working directory
# ALLOWED_WORKING_DIRECTORY=/path/to/your/project

# Model Configuration
# Temperature for LLM responses (0.0 to 1.0)
MODEL_TEMPERATURE=0.1

# Maximum tokens for LLM responses
MAX_TOKENS=4000

# Request timeout in seconds
REQUEST_TIMEOUT=60
