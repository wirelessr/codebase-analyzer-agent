"""
Integration test for Task Specialist with real LLM and review functionality.

This test verifies that the Task Specialist actually interacts with a real LLM
to review analysis reports and provide meaningful feedback based on prompts.
"""

import pytest
import asyncio
from typing import Tuple

from codebase_agent.agents.task_specialist import TaskSpecialist
from codebase_agent.config.configuration import ConfigurationManager


class TestTaskSpecialistIntegration:
    """Integration test for Task Specialist with real LLM interaction."""
    
    @pytest.fixture
    def real_config(self):
        """Load real LLM configuration."""
        try:
            config_manager = ConfigurationManager()
            config_manager.load_environment()
            
            # Use the configuration manager to get model client
            return config_manager.get_model_client()
        except Exception as e:
            pytest.skip(f"Could not configure LLM: {e}")
    
    @pytest.fixture
    def task_specialist(self, real_config):
        """Create Task Specialist with real configuration."""
        return TaskSpecialist(real_config)
    
    def test_specialist_basic_functionality(self, task_specialist):
        """Test basic Task Specialist functionality with simpler assertion."""
        
        task_description = "Simple test task"
        analysis = "Basic analysis content"
        
        print("ðŸš€ Testing basic specialist functionality...")
        
        # Test that the method can be called without errors
        try:
            is_complete, feedback, confidence = task_specialist.review_analysis(
                analysis_report=analysis,
                task_description=task_description,
                current_review_count=1
            )
            
            print(f"ðŸ“Š Basic Test Result:")
            print(f"   Complete: {is_complete}")
            print(f"   Confidence: {confidence:.2f}")
            print(f"   Feedback length: {len(feedback)}")
            
            # Just verify we get some response
            assert isinstance(is_complete, bool), f"Expected boolean, got {type(is_complete)}"
            assert isinstance(feedback, str), f"Expected string, got {type(feedback)}"
            assert isinstance(confidence, (int, float)), f"Expected number, got {type(confidence)}"
            assert len(feedback) > 0, "Expected non-empty feedback"
            
            print("âœ… Basic functionality test passed!")
            
        except Exception as e:
            print(f"âŒ Basic functionality test failed: {e}")
            assert False, f"Basic functionality failed: {e}"
    
    def test_specialist_reviews_incomplete_analysis(self, task_specialist):
        """Test that specialist correctly identifies incomplete analysis."""
        
        task_description = """
        Add user authentication system to a Flask web application.
        The system should support login/logout and session management.
        """
        
        # Deliberately incomplete analysis
        incomplete_analysis = """
        I found some files in the project:
        - app.py (main Flask application)
        - models.py (contains User model)
        - templates/ (HTML templates)
        
        The app.py file imports Flask and creates an app instance.
        """
        
        print("ðŸš€ Testing specialist review of incomplete analysis...")
        
        is_complete, feedback, confidence = task_specialist.review_analysis(
            analysis_report=incomplete_analysis,
            task_description=task_description,
            current_review_count=1
        )
        
        print(f"ðŸ“Š Review Result:")
        print(f"   Complete: {is_complete}")
        print(f"   Confidence: {confidence:.2f}")
        print(f"   Feedback: {feedback}")
        
        # Verify that the specialist correctly identified incompleteness
        assert not is_complete, \
            f"Expected incomplete analysis to be rejected, but was accepted. Feedback: {feedback}"
        
        assert len(feedback) > 50, \
            f"Expected detailed feedback, got: {feedback}"
        
        # Feedback should mention missing areas related to authentication
        feedback_lower = feedback.lower()
        auth_terms = ['authentication', 'auth', 'login', 'session', 'security', 'implementation']
        has_auth_focus = any(term in feedback_lower for term in auth_terms)
        
        assert has_auth_focus, \
            f"Expected feedback to focus on authentication implementation gaps, got: {feedback}"
        
        print("âœ… Test passed: Specialist correctly identified incomplete analysis!")
    
    def test_specialist_accepts_complete_analysis(self, task_specialist):
        """Test that specialist accepts thorough, complete analysis."""
        
        task_description = """
        Add user authentication system to a Flask web application.
        The system should support login/logout and session management.
        """
        
        # Comprehensive analysis
        complete_analysis = """
        EXISTING FUNCTIONALITY ANALYSIS:
        - Found Flask app structure in app.py with basic routing
        - User model exists in models.py with SQLAlchemy ORM
        - Database configuration present using SQLite
        - Templates directory with basic HTML structure
        - Static assets folder for CSS/JS
        
        INTEGRATION POINTS:
        - User model can be extended with authentication fields (password_hash, is_active)
        - Flask-Login can integrate with existing User model
        - Current route structure in app.py allows adding auth decorators
        - Database schema can accommodate auth tables via migrations
        
        IMPLEMENTATION RECOMMENDATIONS:
        1. Install Flask-Login and Werkzeug for password hashing
        2. Add authentication fields to User model (password_hash, last_login)
        3. Create login/logout routes with form validation
        4. Implement session management using Flask-Login's login_manager
        5. Add @login_required decorators to protected routes
        6. Create login/logout templates extending existing base template
        
        POTENTIAL CONFLICTS:
        - No conflicts detected with existing codebase structure
        - Current User model schema needs migration for new auth fields
        - Session configuration may need updating for security
        
        CODE PATTERNS FROM CODEBASE:
        - Existing routes follow pattern: @app.route('/path', methods=['GET', 'POST'])
        - Database models use SQLAlchemy declarative syntax
        - Templates use Jinja2 with base template inheritance
        - Error handling follows Flask's standard abort() pattern
        
        ARCHITECTURE COMPATIBILITY:
        - Flask app factory pattern not used (simple app instance)
        - SQLAlchemy models follow standard conventions
        - No existing middleware that would conflict with Flask-Login
        - Static file serving compatible with auth system
        
        DEPENDENCIES:
        - Flask-Login for session management (compatible with current Flask version)
        - Werkzeug for password hashing (already included with Flask)
        - No breaking changes to existing dependencies required
        """
        
        print("ðŸš€ Testing specialist review of complete analysis...")
        
        is_complete, feedback, confidence = task_specialist.review_analysis(
            analysis_report=complete_analysis,
            task_description=task_description,
            current_review_count=1
        )
        
        print(f"ðŸ“Š Review Result:")
        print(f"   Complete: {is_complete}")
        print(f"   Confidence: {confidence:.2f}")
        print(f"   Feedback: {feedback}")
        
        # With a thorough analysis like this, the LLM might still find areas for improvement
        # This is actually good behavior - being thorough and security-conscious
        if is_complete:
            print("âœ… LLM accepted the comprehensive analysis")
            assert confidence > 0.7, f"Expected high confidence for accepted analysis, got: {confidence}"
        else:
            print("âš ï¸  LLM found additional areas for improvement (thorough review)")
            # Ensure the feedback is substantial and relevant
            assert len(feedback) > 50, f"Expected detailed feedback, got: {feedback}"
            
            # For security-sensitive tasks like authentication, it's reasonable for LLM to be strict
            feedback_lower = feedback.lower()
            quality_terms = ['security', 'detail', 'specific', 'consideration', 'integration', 'implementation']
            has_quality_focus = any(term in feedback_lower for term in quality_terms)
            
            assert has_quality_focus, f"Expected quality-focused feedback, got: {feedback}"
            
            # High-quality rejection should have reasonable confidence
            assert confidence > 0.5, f"Expected reasonable confidence for quality feedback, got: {confidence}"
        
        assert len(feedback) > 20, f"Expected meaningful feedback, got: {feedback}"
        print("âœ… Test passed: Specialist provided appropriate analysis review!")
    
    def test_specialist_multiple_reviews_progressive_acceptance(self, task_specialist):
        """Test that specialist enforces maximum review limit."""
        
        task_description = """
        Implement a simple calculator function.
        """
        
        # Minimal analysis that should normally be rejected
        minimal_analysis = """
        Found some Python files. There are functions in the code.
        """
        
        print("ðŸš€ Testing specialist multiple review behavior...")
        
        # Test progressive reviews
        for review_num in range(1, 4):
            is_complete, feedback, confidence = task_specialist.review_analysis(
                analysis_report=minimal_analysis,
                task_description=task_description,
                current_review_count=review_num
            )
            
            print(f"ðŸ“Š Review {review_num} Result:")
            print(f"   Complete: {is_complete}")
            print(f"   Confidence: {confidence:.2f}")
            print(f"   Feedback: {feedback[:100]}...")
            
            if review_num < 3:
                # First two reviews should likely reject minimal analysis
                # (though we allow for LLM variability)
                print(f"   Review {review_num} decision: {'ACCEPT' if is_complete else 'REJECT'}")
            else:
                # Third review should force accept due to max review limit
                assert is_complete, \
                    f"Expected force accept on review 3, but got rejection. Feedback: {feedback}"
                assert "maximum" in feedback.lower() or "limit" in feedback.lower(), \
                    f"Expected force accept message, got: {feedback}"
        
        print("âœ… Test passed: Specialist correctly handles multiple reviews!")
    
    def test_specialist_llm_prompt_effectiveness(self, task_specialist):
        """Test that the specialist's prompts lead to consistent LLM behavior."""
        
        test_cases = [
            {
                "name": "Database Integration Task",
                "task": "Add PostgreSQL database integration to existing Flask app",
                "analysis": "Found app.py file with Flask imports. Database not analyzed.",
                "expected_complete": False,
                "should_mention": ["database", "postgresql", "integration", "connection"]
            },
            {
                "name": "API Endpoint Task", 
                "task": "Create REST API endpoints for user management",
                "analysis": """
                EXISTING ANALYSIS:
                - Found Flask app with routing structure in app.py
                - User model exists with proper SQLAlchemy setup
                - JSON serialization methods present in models
                
                INTEGRATION POINTS:
                - Can extend existing route patterns for API endpoints
                - User model ready for CRUD operations
                
                IMPLEMENTATION STEPS:
                1. Add API blueprint for user routes
                2. Implement GET /users, POST /users, PUT /users/<id>, DELETE /users/<id>
                3. Add JSON response formatting
                4. Include proper HTTP status codes and error handling
                5. Add request validation and authentication middleware
                6. Document API endpoints with proper error responses
                
                SECURITY CONSIDERATIONS:
                - Input validation for all endpoints
                - Authentication/authorization for protected routes
                - Rate limiting considerations
                
                PATTERNS FROM CODEBASE:
                - Routes use @app.route decorator pattern
                - Database queries follow SQLAlchemy ORM patterns
                - Error handling uses Flask's abort() function
                """,
                "expected_complete": True,  # This more comprehensive analysis should be accepted
                "should_mention": ["accept", "sufficient", "implementation"]
            }
        ]
        
        print("ðŸš€ Testing specialist LLM prompt effectiveness...")
        
        for i, case in enumerate(test_cases, 1):
            print(f"\nðŸ“ Test Case {i}: {case['name']}")
            
            is_complete, feedback, confidence = task_specialist.review_analysis(
                analysis_report=case["analysis"],
                task_description=case["task"],
                current_review_count=1
            )
            
            print(f"   Complete: {is_complete}")
            print(f"   Confidence: {confidence:.2f}")
            print(f"   Feedback: {feedback[:200]}...")
            
            # Check if result matches expectation (but allow for LLM variability)
            if case["expected_complete"]:
                # We expect this to be complete
                if is_complete:
                    print(f"   âœ… Correctly accepted as complete")
                else:
                    print(f"   âš ï¸  LLM was stricter than expected (reasonable behavior)")
                    # Even if rejected, ensure feedback is quality-focused
                    feedback_lower = feedback.lower()
                    quality_indicators = ['detail', 'specific', 'security', 'consideration']
                    has_quality_focus = any(indicator in feedback_lower for indicator in quality_indicators)
                    if has_quality_focus:
                        print(f"   âœ… Rejection was based on quality concerns")
            else:
                # We expect this to be incomplete
                if not is_complete:
                    print(f"   âœ… Correctly identified as incomplete")
                else:
                    print(f"   âš ï¸  LLM accepted unexpectedly (LLM variability)")
            
            # Check if feedback mentions relevant terms
            feedback_lower = feedback.lower()
            mentioned_terms = [term for term in case["should_mention"] 
                             if term in feedback_lower]
            
            if mentioned_terms:
                print(f"   âœ… Mentioned relevant terms: {mentioned_terms}")
            else:
                print(f"   âš ï¸  Expected terms not prominently featured: {case['should_mention']}")
            
            # Verify feedback is substantial and meaningful
            assert len(feedback) > 30, \
                f"Expected substantial feedback, got: {feedback}"
            
            # Ensure confidence is reasonable
            assert 0.0 <= confidence <= 1.0, \
                f"Confidence should be between 0 and 1, got: {confidence}"
        
        print("\nâœ… Test passed: Specialist LLM prompts show effective behavior!")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
